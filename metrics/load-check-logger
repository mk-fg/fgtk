#!/usr/bin/env python

import os, sys, re, time, math, enum, heapq, pathlib as pl


_td_days = dict(
	y=365.2422, yr=365.2422, year=365.2422,
	mo=30.5, month=30.5, w=7, week=7, d=1, day=1 )
_td_s = dict( h=3600, hr=3600, hour=3600,
	m=60, min=60, minute=60, s=1, sec=1, second=1 )
_td_usort = lambda d: sorted(
	d.items(), key=lambda kv: (kv[1], len(kv[0])), reverse=True )
_td_re = re.compile('(?i)^[-+]?' + ''.join( fr'(?P<{k}>\d+{k}\s*)?'
	for k, v in [*_td_usort(_td_days), *_td_usort(_td_s)] ) + '$')

def td_parse(td_str):
	try: return float(td_str)
	except: td = 0
	if (m := _td_re.search(td_str)) and any(m.groups()):
		# Short time offset like "3d 5h"
		for n, units in enumerate((_td_days, _td_s)):
			tdx = 0
			for k, v in units.items():
				if not m.group(k): continue
				tdx += v * int(''.join(filter(str.isdigit, m.group(k))) or 1)
			td += (24*3600)**(1-n) * tdx
		return td
	if m := re.search(r'^\d{1,2}:\d{2}(?::\d{2}(?P<us>\.\d+)?)?$', td_str):
		# [[HH:]MM:]SS where seconds can be fractional
		return sum(n*float(m) for n,m in zip((3600, 60, 1), td_str.split(':')))
	raise ValueError(f'Failed to parse time-delta spec: {td_str}')


def token_bucket(spec, negative_tokens=False):
	'''Spec: { td: float | float_a/float_b }[:burst_float]
			Examples: 1/4:5 (td=0.25s, rate=4/s, burst=5), 5, 0.5:10, 20:30.
		Expects a number of tokens (can be float, def=1),
			and subtracts these, borrowing below zero with negative_tokens set.
		Yields either None if there's enough tokens
			or delay (in seconds, float) until when there will be.
		Simple "0" value means "always None", or "inf" for "infinite delays".'''
	try:
		try: interval, burst = spec.rsplit(':', 1)
		except (ValueError, AttributeError): interval, burst = spec, 1.0
		else: burst = float(burst)
		if isinstance(interval, str):
			try: a, b = interval.split('/', 1)
			except ValueError: interval = td_parse(interval)
			else: interval = td_parse(a) / float(b)
		if min(interval, burst) < 0: raise ValueError
	except: raise ValueError(f'Invalid format for rate-limit: {spec!r}')
	if interval <= 0:
		while True: yield None
	elif interval == math.inf:
		while True: yield 2**31 # safe "infinite delay" value
	tokens, rate, ts_sync = max(0, burst - 1), interval**-1, time.monotonic()
	val = (yield) or 1
	while True:
		ts = time.monotonic()
		ts_sync, tokens = ts, min(burst, tokens + (ts - ts_sync) * rate)
		val, tokens = ( (None, tokens - val) if tokens >= val else
			((val - tokens) / rate, (tokens - val) if negative_tokens else tokens) )
		val = (yield val) or 1


p = lambda *a,**kw: print(*a, **kw, flush=True)
p_load = lambda *a,**kw: print('LOAD ::', *a, **kw, file=sys.stderr, flush=True)
p_err = lambda *a,**kw: print('ERROR:', *a, **kw, file=sys.stderr, flush=True) or 1
err_fmt = lambda err: f'[{err.__class__.__name__}] {err}'

ct = enum.Enum('CheckType', 'la nproc')


def run_check_la( ts, caches, t, td, cap, tbf, p_src=pl.Path('/proc/loadavg'),
		la_fmt=lambda las: '/'.join(f'{la or 0:.2f}'.rstrip('0').rstrip('.') for la in las) ):
	if (la := caches.get(p_src)) and la[0] == ts: la = la[1]
	else: caches[p_src] = ts, (la := p_src.read_text())
	for c, v in zip(cap, la := list(float(v) for v in la.split(' ', 4)[:3])):
		if v > c: break
	else: return
	if not tbf or not next(tbf): p_load( 'load-averages above'
		f' 1/5/15 {"tbf-" if tbf else ""}caps: {la_fmt(la)} > {la_fmt(cap)}' )

def run_check_nproc(ts, caches, t, td, cap, tbf, p_src=pl.Path('/proc/loadavg')):
	if (la := caches.get(p_src)) and la[0] == ts: la = la[1]
	else: caches[p_src] = ts, (la := p_src.read_text())
	if (n := int(la.split(' ', 4)[3].split('/', 1)[-1])) <= cap: return
	if not tbf or not next(tbf): p_load(
		f'total process-count above {"tbf-" if tbf else ""}cap: {n:,d} > {cap:,d}' )

def run_checks(td_checks):
	chk_funcs = {ct.la: run_check_la, ct.nproc: run_check_nproc}
	q, ts, caches = list(), time.monotonic(), dict()
	for td in td_checks: heapq.heappush(q, (ts + td, td))
	while True:
		ts_chk, td = q[0]
		if delay := max(0, ts_chk - time.monotonic()):
			p(f'--- delay until next check: {delay:.1f}'); time.sleep(delay)
		heapq.heappushpop(q, ((ts := time.monotonic()) + td, td))
		for chk in td_checks[td]: chk_funcs[chk['t']](ts, caches, **chk)


def main(argv=None):
	import argparse, textwrap
	dd = lambda text: re.sub( r' \t+', ' ',
		textwrap.dedent(text).strip('\n') + '\n' ).replace('\t', '  ')
	parser = argparse.ArgumentParser(
		formatter_class=argparse.RawTextHelpFormatter, description=dd('''
			Check various system load values and simply log to stderr if above thresholds.
			Most threshold options can be specified multiple times, to be checked separately.
			All sampling intervals, or other otherwise-undocumented intervals can be specified
				using short time-delta notation (e.g. "30s", "10min", "1h 20m", "1d12h5m"),
				or the usual [[HH:]MM:]SS where seconds can be fractional too.'''))

	group = parser.add_argument_group('Optional thresholds to check')
	group.add_argument('-l', '--loadavg', action='append',
		metavar='sample-interval:[max1]/[max5]/[max15][:tbf-interval[/rate][:burst]]', help=dd('''
			Thresholds for any /proc/loadavg numbers, to be checked for each sample-interval.
			If token-bucket parameters are specified at the end,
				load warnings are only logged when that rate-limit is exceeded,
				i.e. load consistently is above thresholds for some time.
			"tbf-interval[/rate]" should represent an interval (in seconds or as time-delta spec),
				e.g.: 1/4:5 (interval=0.25s, rate=4/s, burst=5), 5 (burst=1), 0.5:10, 1h30m/8:30, etc.'''))
	group.add_argument('-p', '--nproc', action='append',
		metavar='sample-interval:max-procs[:tbf-interval[/rate][:burst]]', help=dd('''
			Check count of total pids in /proc/loadavg for each sample-interval.
			Same optional token-bucket rate-threshold and format as in -l/--loadavg.'''))

	group = parser.add_argument_group('General options')
	group.add_argument('-r', '--max-rate-tbf', metavar='interval[/rate][:burst]', help=dd('''
		Max rate of emitted notifications, where any ones exceeding it will be dropped.
		This is applied across all over-threshold warnings from this script,
			so is a fairly crude filter, where spammy ones can easily drown-out the rest.
		Token-bucket filter parameters are same as in various checks, see e.g. -l/--loadavg.'''))
	group.add_argument('-d', '--debug', action='store_true', help='Verbose operation mode')

	opts = parser.parse_args(argv)

	if not opts.debug: global p; p = lambda *a,**kw: None
	if opts.max_rate_tbf:
		global p_load; p_load_tbf = token_bucket(opts.max_rate_tbf)
		p_load_raw, p_load = p_load, lambda *a,**kw: next(p_load_tbf) or p_load_raw(*a,**kw)

	checks = dict()
	for chk in opts.loadavg or list():
		try:
			td, s, cap = chk.partition(':'); cap, s, tbf = cap.partition(':')
			td, tbf = td_parse(td), tbf and token_bucket(tbf)
			cap = list(float(n) if n else 0 for n in cap.split('/'))
		except Exception as err:
			parser.error(f'Failed to parse -l/--loadavg check [ {chk!r} ]: {err_fmt(err)}')
		checks.setdefault(td, list()).append(dict(t=ct.la, td=td, cap=cap, tbf=tbf))
	for chk in opts.nproc or list():
		try:
			td, s, cap = chk.partition(':'); cap, s, tbf = cap.partition(':')
			td, cap, tbf = td_parse(td), int(cap), tbf and token_bucket(tbf)
		except Exception as err:
			parser.error(f'Failed to parse -n/--nproc check [ {chk!r} ]: {err_fmt(err)}')
		checks.setdefault(td, list()).append(dict(t=ct.nproc, td=td, cap=cap, tbf=tbf))

	if not checks: parser.error('No thresholds/checks specified')
	p(f'Starting monitoring {sum(len(td_chks) for td_chks in checks.values())} checks...')
	run_checks(checks)
	p('Finished')

if __name__ == '__main__': sys.exit(main())
